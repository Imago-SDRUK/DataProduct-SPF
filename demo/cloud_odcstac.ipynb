{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c7066e-336b-489a-bd7c-55b43b9c4680",
   "metadata": {},
   "source": [
    "This Notebook is the raw code to get the cloud probability product without much description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98d8367-076f-46ed-9f69-6a0d0ce32cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install odc-stac\n",
    "# pip install pystac-client\n",
    "# pip install dask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pystac_client import Client\n",
    "import geopandas as gpd\n",
    "import rioxarray as xrx\n",
    "\n",
    "import odc.stac\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "tiles=\"data/uk_20km_grid.gpkg\"\n",
    "tile_of_interest=\"NZ44\" #NOTE: NZ26 is the main one\n",
    "collection = \"sentinel-2-c1-l2a\"\n",
    "time_of_interest = '2024-01-01/2024-12-31'\n",
    "bands_of_interest = [\"cloud\", \"scl\"]\n",
    "api_url=\"https://earth-search.aws.element84.com/v1\"\n",
    "res = 20\n",
    "target_crs=27700\n",
    "\n",
    "tiles = gpd.read_file(tiles).to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ccb848-d081-45b7-83fb-fe36297487b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address space (virtual memory) soft/hard: -1/-1\n",
      "Data segment size soft/hard: -1/-1\n"
     ]
    }
   ],
   "source": [
    "def check_kernel_limit():\n",
    "    \"\"\"This checks the local kernel limitations on memory.\n",
    "    Prints `-1` if inherited from OS and no restrictions.\"\"\"\n",
    "    import resource\n",
    "\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    print(f\"Address space (virtual memory) soft/hard: {soft}/{hard}\")\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_DATA)\n",
    "    print(f\"Data segment size soft/hard: {soft}/{hard}\")\n",
    "\n",
    "check_kernel_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c0d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_search(items,index=0):\n",
    "    \"\"\"Print inspect/debug info about the one of the assets (usually the first one).\n",
    "    Parameters:\n",
    "    items (pystac.ItemCollection): STAC collection\n",
    "    index (int=0): random tile number to inspect (Default: 0)\n",
    "    \"\"\"\n",
    "    item = items[index]\n",
    "    try:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Inspecting the asset #{index}\")\n",
    "        print(f\"DATETIME is {item.datetime}\")\n",
    "        print(f\"GEOMETRY is {item.geometry}\")\n",
    "        print(f\"PROPERTIES are:\\n{item.properties}\")\n",
    "        print(f\"CRS: {item.properties.get('proj:code') or item.properties.get('proj:epsg')}\")\n",
    "        metadata=odc.stac.extract_collection_metadata(item, cfg=None, md_plugin=None)\n",
    "        print(f\"STAC metadata:\\n{metadata}\")\n",
    "        print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Error checking item[{index}]: {e}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b676b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_with_scl(data):\n",
    "    \"\"\"\n",
    "    Replace cloud values based on SCL mask.\n",
    "    Rules:\n",
    "    - If SCL == 0 → cloud = -1\n",
    "    - If SCL != 0 → keep cloud as is\n",
    "    - Assign -1 as the new no-data value for the cloud band\n",
    "    - Ensure no NaNs remain (fill them with -1)\n",
    "    \n",
    "    Parameters:\n",
    "    data (xarray.Dataset): original dataset with `cloud` and `scl` variables\n",
    "    Returns:\n",
    "    cloud (xarray.DataArray): output array with masked `cloud` band\n",
    "    \"\"\"\n",
    "    cloud = data[\"cloud\"].astype(\"float32\").copy()\n",
    "    scl = data[\"scl\"].astype(\"int16\")\n",
    "\n",
    "    # apply rules\n",
    "    cloud = cloud.where(scl != 0, -1)\n",
    "\n",
    "    # replace any remaining NaNs with -1\n",
    "    cloud = cloud.fillna(-1)\n",
    "    # assign -1 as the nodata value for output\n",
    "    cloud.attrs[\"nodata\"] = -1\n",
    "\n",
    "    print(f\"Cloud dimensions: {cloud.dims}\")\n",
    "    print(f\"SCL dimensions: {scl.dims}\")\n",
    "    print(\"-1 is set as the no-data value (no NaNs remain).\")\n",
    "\n",
    "    return cloud\n",
    "\n",
    "    \"\"\"\n",
    "    #NOTE: DEBUG for checking the cloud masked \n",
    "    #NOTE: heavy calculation as it opens the whole numpy array (computes)\n",
    "    num_nodata = cloud_masked.isnull().sum().compute().item()\n",
    "    print(f\"Number of no-data (NaN) values: {num_nodata}\")\n",
    "    \n",
    "    # select the first time slice if 'time' is one of the dimensions\n",
    "    if \"time\" in cloud_masked.dims:\n",
    "        first_scene = cloud_masked.isel(time=0)\n",
    "    else:\n",
    "        first_scene = cloud_masked\n",
    "\n",
    "    # ensure CRS and spatial transform are defined\n",
    "    first_scene = first_scene.rio.write_crs(data[\"cloud\"].rio.crs, inplace=False)\n",
    "    # export to GeoTIFF\n",
    "    output_path = \"cloud_masked_first_scene.tif\"\n",
    "    first_scene.rio.to_raster(output_path)\n",
    "    print(f\"Exported first scene to {output_path}\")\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # DEPRECATED\n",
    "    # define bad pixels\n",
    "    nodatas = bitmask\n",
    "    all_bad_pixels = nodatas(dim=\"time\")\n",
    "\n",
    "    # Expand dimensions to match data shape\n",
    "    all_bad_expanded = all_bad_pixels.broadcast_like(nodatas)\n",
    "\n",
    "    # For these pixels, we’ll override and mark them as good\n",
    "    effective_bad_mask = nodatas.where(~all_bad_expanded, other=False)\n",
    "\n",
    "    # Apply the mask: keep data where bad == False\n",
    "    masked = data.where(~effective_bad_mask)\n",
    "    return masked\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1776bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_geotif(dataset,bands_of_interest:list=None, out_path:str=None):\n",
    "    \"\"\"The function saves the dataset with several variables to geotiff\"\"\"\n",
    "    # Taken from: https://discourse.pangeo.io/t/comparing-odc-stac-load-and-stackstac-for-raster-composite-workflow/4097\n",
    "\n",
    "    if isinstance(dataset, xr.DataArray):\n",
    "        print(f\"dataarray\")\n",
    "        image = dataset.squeeze('year').rio.write_crs(target_crs)\n",
    "\n",
    "        # NOTE: this might overflow dask\n",
    "        '''num_nodata = image.isnull().sum()\n",
    "        print(f\"Number of nodata pixels: {num_nodata}\")'''\n",
    "        \n",
    "    else:\n",
    "        image = (\n",
    "            dataset[bands_of_interest]\n",
    "            .to_dataarray(dim=\"band\")\n",
    "            .transpose(..., \"band\")\n",
    "            .squeeze('year')\n",
    "            .transpose('band', 'y', 'x')\n",
    "            .rio.write_crs(f\"epsg:{target_crs}\")\n",
    "        )\n",
    "\n",
    "    return image.rio.to_raster(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e629669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cloud probability for a tile of interest NZ44:\n",
      "----------------------------------------\n",
      "Process for tile NZ44 started.\n",
      "Bbox of interest is: [-1.3800354189419266, 54.75137797895958, -1.0651866799222283, 54.93308063265072]\n",
      "Number of items: 292\n",
      "Type of items: <class 'pystac.item_collection.ItemCollection'>\n",
      "----------------------------------------\n",
      "Inspecting the asset #0\n",
      "DATETIME is 2024-12-31 11:16:09.671000+00:00\n",
      "GEOMETRY is {'type': 'Polygon', 'coordinates': [[[-1.4892868190198094, 55.037602642484046], [-1.945565451302893, 54.055591019680406], [-1.3232267769574286, 54.04852390275896], [-1.2822817967468176, 55.034856954918745], [-1.4892868190198094, 55.037602642484046]]]}\n",
      "PROPERTIES are:\n",
      "{'created': '2024-12-31T16:39:31.284Z', 'platform': 'sentinel-2a', 'constellation': 'sentinel-2', 'instruments': ['msi'], 'eo:cloud_cover': 100, 'proj:centroid': {'lat': 54.46203, 'lon': -1.53085}, 'mgrs:utm_zone': 30, 'mgrs:latitude_band': 'U', 'mgrs:grid_square': 'WF', 'grid:code': 'MGRS-30UWF', 'view:azimuth': 108.6879993147173, 'view:incidence_angle': 10.677728369688815, 'view:sun_azimuth': 166.952436150102, 'view:sun_elevation': 11.490431784751095, 's2:tile_id': 'S2A_OPER_MSI_L2A_TL_2APS_20241231T152446_A049755_T30UWF_N05.11', 's2:degraded_msi_data_percentage': 0.0454, 's2:nodata_pixel_percentage': 75.342125, 's2:saturated_defective_pixel_percentage': 0, 's2:cloud_shadow_percentage': 0, 's2:vegetation_percentage': 0, 's2:not_vegetated_percentage': 0, 's2:water_percentage': 0, 's2:unclassified_percentage': 0, 's2:medium_proba_clouds_percentage': 1.835155, 's2:high_proba_clouds_percentage': 98.164845, 's2:thin_cirrus_percentage': 0, 's2:snow_ice_percentage': 0, 's2:product_type': 'S2MSI2A', 's2:processing_baseline': '05.11', 's2:product_uri': 'S2A_MSIL2A_20241231T111451_N0511_R137_T30UWF_20241231T152446.SAFE', 's2:generation_time': '2024-12-31T15:24:46.000000Z', 's2:datatake_id': 'GS2A_20241231T111451_049755_N05.11', 's2:datatake_type': 'INS-NOBS', 's2:datastrip_id': 'S2A_OPER_MSI_L2A_DS_2APS_20241231T152446_S20241231T111453_N05.11', 's2:reflectance_conversion_factor': 1.03408943229423, 'datetime': '2024-12-31T11:16:09.671000Z', 'earthsearch:payload_id': 'roda-sentinel-2-c1-l2a/workflow-sentinel-2-c1-l2a-to-stac/9f44e8f33c1ff325994919e5fa6f8cb8', 'storage:platform': 'AWS', 'storage:region': 'us-west-2', 'storage:requester_pays': False, 'processing:software': {'sentinel-2-c1-l2a-to-stac': 'v2024.02.01'}, 'updated': '2024-12-31T16:39:31.284Z', 'proj:code': 'EPSG:32630'}\n",
      "CRS: EPSG:32630\n",
      "STAC metadata:\n",
      "RasterCollectionMetadata(name='sentinel-2-c1-l2a', meta=RasterGroupMetadata(bands={('red', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('green', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('blue', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('visual', 1): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=()), ('visual', 2): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=()), ('visual', 3): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=()), ('nir', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('swir22', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('rededge2', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('rededge3', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('rededge1', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('swir16', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('wvp', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='cm', dims=()), ('nir08', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('scl', 1): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=()), ('aot', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('coastal', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('nir09', 1): RasterBandMetadata(data_type='uint16', nodata=0, units='1', dims=()), ('cloud', 1): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=()), ('snow', 1): RasterBandMetadata(data_type='uint8', nodata=0, units='1', dims=())}, aliases={'B04': [('red', 1), ('preview', 1), ('visual', 1)], 'B03': [('green', 1), ('preview', 2), ('visual', 2)], 'B02': [('blue', 1), ('preview', 3), ('visual', 3)], 'B08': [('nir', 1)], 'B12': [('swir22', 1)], 'B06': [('rededge2', 1)], 'rededge': [('rededge1', 1), ('rededge2', 1), ('rededge3', 1)], 'B07': [('rededge3', 1)], 'B05': [('rededge1', 1)], 'B11': [('swir16', 1)], 'B8A': [('nir08', 1)], 'B01': [('coastal', 1)], 'B09': [('nir09', 1)]}, extra_dims={}, extra_coords=()), has_proj=True, band2grid={'red': 'g10', 'green': 'g10', 'blue': 'g10', 'visual': 'g10', 'nir': 'g10', 'swir22': 'default', 'rededge2': 'default', 'rededge3': 'default', 'rededge1': 'default', 'swir16': 'default', 'wvp': 'default', 'nir08': 'default', 'scl': 'default', 'aot': 'default', 'cloud': 'default', 'snow': 'default', 'coastal': 'g60', 'nir09': 'g60'})\n",
      "----------------------------------------\n",
      "<xarray.Dataset> Size: 612MB\n",
      "Dimensions:      (y: 1024, x: 1023, time: 292)\n",
      "Coordinates:\n",
      "  * y            (y) float64 8kB 5.602e+05 5.602e+05 ... 5.398e+05 5.398e+05\n",
      "  * x            (x) float64 8kB 4.398e+05 4.398e+05 ... 4.602e+05 4.603e+05\n",
      "    spatial_ref  int32 4B 27700\n",
      "  * time         (time) datetime64[ns] 2kB 2024-01-01T11:16:05.954000 ... 202...\n",
      "Data variables:\n",
      "    cloud        (time, y, x) uint8 306MB dask.array<chunksize=(20, 300, 300), meta=np.ndarray>\n",
      "    scl          (time, y, x) uint8 306MB dask.array<chunksize=(20, 300, 300), meta=np.ndarray>\n",
      "FrozenMappingWarningOnValuesAccess({'y': 1024, 'x': 1023, 'time': 292})\n"
     ]
    }
   ],
   "source": [
    "if tile_of_interest: \n",
    "    print(f\"Calculating cloud probability for a tile of interest {tile_of_interest}:\")\n",
    "    tile_name = tile_of_interest\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f'Process for tile {tile_name} started.', flush=True)\n",
    "    \n",
    "    selected_tile = tiles[tiles[\"tile_name\"]==tile_of_interest]\n",
    "    bbox_of_interest = selected_tile.total_bounds.tolist()\n",
    "    print(f\"Bbox of interest is: {bbox_of_interest}\")\n",
    "\n",
    "    catalog = Client.open(\n",
    "    api_url\n",
    "    )\n",
    "    search = catalog.search(\n",
    "        collections=collection,\n",
    "        bbox=bbox_of_interest,\n",
    "        datetime=time_of_interest\n",
    "    )\n",
    "    items = search.item_collection()\n",
    "    print(f\"Number of items: {len(items)}\")\n",
    "    print(f\"Type of items: {type(items)}\")\n",
    "    inspect_search(items, index=0)\n",
    "\n",
    "    data = odc.stac.stac_load(\n",
    "        items, bands=bands_of_interest, \n",
    "        bbox=bbox_of_interest,\n",
    "        resolution=res,\n",
    "        crs=target_crs,\n",
    "        chunks={'time': 20, 'x': 300, 'y': 300}\n",
    "        #align=target_resolution_grid  # optional, aligns to custom grid # NOTE: ODC automatically extends the data from the bbox by a few pixels to each side\n",
    "    ) \n",
    "    # NOTE: odc.stac.stac_load is not covered by the documentation yet (only the old version - odc.stac.load in 0.39.0)\n",
    "    # TODO - to check docs 0.40.0\n",
    "\n",
    "    print(data)\n",
    "    print(data.dims) \n",
    "    \n",
    "    # NOTE: TODO - for further development MULTIPLE TILES\n",
    "    \"\"\"for idx, row in tiles.bounds.iterrows():\n",
    "    tile_name = tiles.loc[row.name, \"tile_name\"]\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f'Process for tile {tile_name} started.', flush=True)\n",
    "\n",
    "    bbox_of_interest = row.to_list()\n",
    "    print(bbox_of_interest)\n",
    "\n",
    "    catalog = Client.open(\n",
    "    api_url\n",
    "    )\n",
    "    search = catalog.search(\n",
    "        collections=collection,\n",
    "        bbox=bbox_of_interest,\n",
    "        datetime=time_of_interest\n",
    "    )\n",
    "    items = search.item_collection()\n",
    "    inspect_search(items, index=0)\n",
    "\n",
    "    data = odc.stac.stac_load(\n",
    "        items, bands=bands_of_interest, \n",
    "        bbox=bbox_of_interest,\n",
    "        resolution=res,\n",
    "        crs=target_crs\n",
    "    )\n",
    "\n",
    "    print(data)\n",
    "    print(data.dims)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0aa1bee-6aec-47e7-900b-b593c8dc8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_s2_scenes(data, items, output_dir=\"data/test\", band=\"cloud\"):\n",
    "    \"\"\"\n",
    "    Export Sentinel-2 scenes from an xarray Dataset or DataArray to individual GeoTIFFs.\n",
    "    Useful for visual checks in \n",
    "    Filenames are based on the STAC 's2:tile_id' property.\"\"\"\n",
    "    \n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # extract tile IDs from STAC items\n",
    "    tile_ids = [item.properties.get(\"s2:tile_id\", f\"scene_{i}\") for i, item in enumerate(items)]\n",
    "    # attach as the coordinate\n",
    "    if \"tile_id\" not in data.coords:\n",
    "        data = data.assign_coords(tile_id=(\"time\", tile_ids))\n",
    "    # wrap in a dataset if it's dataarray\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        print(\"Input is a DataArray — converting to Dataset for export.\")\n",
    "        data = data.to_dataset(name=data.name or band)\n",
    "    # if band exists\n",
    "    if band not in data.data_vars:\n",
    "        raise ValueError(f\"Band '{band}' not found in dataset. Available bands: {list(data.data_vars)}\")\n",
    "        \n",
    "    # loop over scenes\n",
    "    for i, tile_id in enumerate(data.tile_id.values):\n",
    "        print(f\"Processing scene {i+1}/{len(data.time)} → {tile_id}\")\n",
    "\n",
    "        # select one scene and load into memory\n",
    "        scene = data.isel(time=i).compute()\n",
    "        out_path = os.path.join(output_dir, f\"{band}_{tile_id}.tif\")\n",
    "        # to check if crs is written\n",
    "        scene_band = scene[band]\n",
    "        scene_band = scene_band.rio.write_crs(scene_band.rio.crs or data[band].rio.crs, inplace=False)\n",
    "        \n",
    "        scene_band.rio.to_raster(out_path)\n",
    "        print(f\"Exported: {out_path}\")\n",
    "\n",
    "    print(f\"\\n All {len(data.time)} scenes exported to '{output_dir}'.\")\n",
    "\n",
    "# USAGE (to export unmasked scenes)\n",
    "# export_s2_scenes(data, items, output_dir=\"data/test/unmasked\", band=\"cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e06f79-288c-4723-bf0b-32dfe4b06f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud dimensions: ('time', 'y', 'x')\n",
      "SCL dimensions: ('time', 'y', 'x')\n",
      "-1 is set as the no-data value (no NaNs remain).\n",
      "Cloud masked with SCL.\n",
      "Type: <class 'xarray.core.dataarray.DataArray'>\n",
      "Name: cloud\n",
      "Dimensions: ('time', 'y', 'x')\n",
      "Coordinates: ['y', 'x', 'spatial_ref', 'time']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cloud_masked = mask_with_scl(data)\n",
    "    print(f\"Cloud masked with SCL.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to mask cloud with SCL band\")\n",
    "\n",
    "print(\"Type:\", type(cloud_masked))\n",
    "print(\"Name:\", cloud_masked.name)\n",
    "print(\"Dimensions:\", cloud_masked.dims)\n",
    "print(\"Coordinates:\", list(cloud_masked.coords))\n",
    "\n",
    "#DEBUG: check each scene\n",
    "#export_s2_scenes(cloud_masked, items, output_dir=\"data/test/masked\", band=\"cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d0fc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:27700\n",
      "(439820.0, 539760.0, 460280.0, 560240.0)\n",
      "dataarray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/rasterio/warp.py:387: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dest = _reproject(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output GeoTIFF saved to data/NZ44_2024.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntif = xrx.open_rasterio(\"data/NZ26_2024.tif\", masked=True)  # masked=True treats nodata as NaN\\nnum_nodata = int(tif.isnull().sum())\\ntotal_pixels = tif.size\\npercent_nodata = num_nodata / total_pixels * 100\\n\\nprint(f\"GeoTIFF shape: {tif.shape}\")\\nprint(f\"NoData pixels (NaN): {num_nodata}\")\\nprint(f\"Percent NoData: {percent_nodata:.2f}%\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: version with cloud_prob, masked by scl:\n",
    "# NOTE: do not skip `.where(cloud-masked!=-1)` because otherwise it just won't consider pixels covered by satellite, but with a 0 cloud probability\n",
    "# NOTE: we use now `-1` instead of 0 because that's our new no data value\n",
    "cloud_prob = (\n",
    "    cloud_masked\n",
    "    .where(cloud_masked != -1)\n",
    "    .groupby(\"time.year\")\n",
    "    .mean(dim=\"time\", skipna=True)\n",
    "    .astype('float32')\n",
    ")\n",
    "\n",
    "print(cloud_prob.rio.crs)\n",
    "print(cloud_prob.rio.bounds()) # NOTE: bounds are extended\n",
    "\n",
    "# NOTE: DEBUG check of the output dataset (unique values, 0 values, no data values) - this is all very heavy for Dask computations\n",
    "\"\"\"cloud_prob_2d = cloud_prob.squeeze('year')  # shape: (y, x)\n",
    "unique_values = np.unique(cloud_prob_2d.values)\n",
    "print(f\"Number of unique values: {len(unique_values)}\")\n",
    "print(\"First 20 unique values:\", unique_values[:20])\n",
    "num_zeros = np.sum(cloud_prob_2d.values == 0)\n",
    "total_pixels = cloud_prob_2d.size\n",
    "percent_zeros = num_zeros / total_pixels * 100\n",
    "print(f\"Number of pixels with value 0: {num_zeros}\")\n",
    "print(f\"Percent of zeros: {percent_zeros:.2f}%\")\n",
    "\n",
    "print(cloud_prob.shape)\"\"\"\n",
    "\n",
    "'''\n",
    "cloud_prob_2d = cloud_prob.squeeze('year')  # shape: (y, x)\n",
    "num_zeros = np.sum(cloud_prob_2d.values == 0)\n",
    "total_pixels = cloud_prob_2d.size\n",
    "percent_zeros = num_zeros / total_pixels * 100\n",
    "print(f\"Number of pixels with value 0: {num_zeros}\")\n",
    "print(f\"Percent of zeros: {percent_zeros:.2f}%\")\n",
    "\n",
    "# squeeze year dimension if single year\n",
    "cloud_prob = cloud_prob.squeeze('year')\n",
    "\n",
    "# convert variables to float32\n",
    "cloud_prob = cloud_prob.astype('float32')\n",
    "\n",
    "# set spatial dims for rioxarray\n",
    "cloud_prob = cloud_prob.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "# set CRS\n",
    "cloud_dataset = cloud_prob.rio.write_crs(27700)\n",
    "\n",
    "out_path = \"data/cloud_median_count_2024.tif\"\n",
    "cloud_dataset.rio.to_raster(out_path)\n",
    "print(f\"Saved multi-band GeoTIFF to {out_path}\")'''\n",
    "\n",
    "out_path = f'data/{tile_name}_2024.tif'\n",
    "try:\n",
    "    to_geotif(cloud_prob, bands_of_interest[:-1], out_path=out_path)\n",
    "    print(f\"Output GeoTIFF saved to {out_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save GeoTIFF: {e}\")\n",
    "\n",
    "'''\n",
    "tif = xrx.open_rasterio(\"data/NZ26_2024.tif\", masked=True)  # masked=True treats nodata as NaN\n",
    "num_nodata = int(tif.isnull().sum())\n",
    "total_pixels = tif.size\n",
    "percent_nodata = num_nodata / total_pixels * 100\n",
    "\n",
    "print(f\"GeoTIFF shape: {tif.shape}\")\n",
    "print(f\"NoData pixels (NaN): {num_nodata}\")\n",
    "print(f\"Percent NoData: {percent_nodata:.2f}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f22029-0bb5-478a-adaf-a4d7ff76a777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef crop_array(array, bbox):\\n\"Crops the output array by the initial bounding box.\\nParameters:\\nxarray.DataArray: output array\\nbbox: initial bounding box\\nReturns:\\nxarray.DataArray: cropped array\\n    .rio.clip_box() # to crop by the initial bounding box\\n    \\ncrop_array(cloud_prob,bbox)    \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - decide how to handle cropping tiles in mosaic (as odc stac returns arrays with extended borders)\n",
    "\"\"\"\n",
    "def crop_array(array, bbox):\n",
    "\"Crops the output array by the initial bounding box.\n",
    "Parameters:\n",
    "xarray.DataArray: output array\n",
    "bbox: initial bounding box\n",
    "Returns:\n",
    "xarray.DataArray: cropped array\n",
    "    .rio.clip_box() # to crop by the initial bounding box\n",
    "    \n",
    "crop_array(cloud_prob,bbox)    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac46cf3-afa5-4677-9ba4-1ce281a044c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "(1, 1024, 1023)\n",
      "Coordinates:\n",
      "  * y            (y) float64 8kB 5.602e+05 5.602e+05 ... 5.398e+05 5.398e+05\n",
      "  * x            (x) float64 8kB 4.398e+05 4.398e+05 ... 4.602e+05 4.603e+05\n",
      "    spatial_ref  int32 4B 27700\n",
      "  * year         (year) int64 8B 2024\n",
      "{'nodata': -1}\n",
      "('year', 'y', 'x')\n",
      "Total nodata values: <xarray.DataArray 'cloud' ()> Size: 8B\n",
      "dask.array<sum-aggregate, shape=(), dtype=int64, chunksize=(), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    spatial_ref  int32 4B 27700\n"
     ]
    }
   ],
   "source": [
    "def check_array(array):\n",
    "    \"\"\"General info about the output array\"\"\"\n",
    "    print(type(array))\n",
    "    print(array.shape)\n",
    "    print(array.coords)\n",
    "    print(array.attrs)\n",
    "    print(array.dims)\n",
    "    num_nodata = array.isnull().sum()\n",
    "    print(f\"Total nodata values: {num_nodata}\")\n",
    "\n",
    "check_array(cloud_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3105c24b-1da0-45d5-b5d4-130b9723cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 2 variables:\n",
      "============================================================\n",
      "\n",
      "Variable: cloud\n",
      "------------------------------------------------------------\n",
      "  time: 15 chunks | sizes = (20, 20, 20, 20, 20)...\n",
      "     y: 4 chunks | sizes = (300, 300, 300, 124)\n",
      "     x: 4 chunks | sizes = (300, 300, 300, 123)\n",
      "------------------------------------------------------------\n",
      "Total chunks: 240\n",
      "Average chunk size: 1.27 MB (uint8)\n",
      "Chunks evenly sized? No, uneven chunks\n",
      "\n",
      "Variable: scl\n",
      "------------------------------------------------------------\n",
      "  time: 15 chunks | sizes = (20, 20, 20, 20, 20)...\n",
      "     y: 4 chunks | sizes = (300, 300, 300, 124)\n",
      "     x: 4 chunks | sizes = (300, 300, 300, 123)\n",
      "------------------------------------------------------------\n",
      "Total chunks: 240\n",
      "Average chunk size: 1.27 MB (uint8)\n",
      "Chunks evenly sized? No, uneven chunks\n"
     ]
    }
   ],
   "source": [
    "def inspect_chunks(obj):\n",
    "    \"\"\"\n",
    "    Inspects Dask chunking for an xarray Dataset or DataArray.\n",
    "    Prints total number of chunks, average size, and alignment info.\n",
    "    \"\"\"\n",
    "    # Handle Dataset (multiple variables)\n",
    "    if isinstance(obj, xr.Dataset):\n",
    "        print(f\"Dataset with {len(obj.data_vars)} variables:\")\n",
    "        print(\"=\" * 60)\n",
    "        for var_name, da in obj.data_vars.items():\n",
    "            print(f\"\\nVariable: {var_name}\")\n",
    "            inspect_chunks(da)\n",
    "        return\n",
    "\n",
    "    da = obj #handle dataarray (single variable)\n",
    "\n",
    "    if not hasattr(da.data, \"chunks\"):\n",
    "        print(\"Array not chunked (not a Dask array).\")\n",
    "        return\n",
    "\n",
    "    chunks = da.data.chunks\n",
    "    dtype_size = da.dtype.itemsize\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    total_chunks = 1\n",
    "    uneven = False\n",
    "\n",
    "    for dim, sizes in zip(da.dims, chunks):\n",
    "        total_chunks *= len(sizes)\n",
    "        equal = len(set(sizes)) == 1\n",
    "        if not equal:\n",
    "            uneven = True\n",
    "        print(f\"{dim:>6}: {len(sizes)} chunks | sizes = {sizes[:5]}{'...' if len(sizes) > 5 else ''}\")\n",
    "\n",
    "    avg_chunk_elems = np.prod([np.mean(s) for s in chunks])\n",
    "    avg_chunk_bytes = avg_chunk_elems * dtype_size\n",
    "    avg_chunk_mb = avg_chunk_bytes / 1e6\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_mb:.2f} MB ({da.dtype})\")\n",
    "    print(f\"Chunks evenly sized? {'Yes' if not uneven else 'No, uneven chunks'}\")\n",
    "\n",
    "inspect_chunks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4b2cad1-97b7-4c7a-ab2f-d2ad3b613de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "def get_chunk_polygons(da, var_name=None):\n",
    "    \"\"\"\n",
    "    Generate a geodataframe with polygons for each Dask chunk of a dataarray.\n",
    "    Parameters:\n",
    "    da (xr.DataArray): dask-backed DataArray with 'x' and 'y' coordinates\n",
    "    var_name (str, optional): variable name for labeling in the geodataframe\n",
    "    Returns:\n",
    "    gdf (geopandas.GeoDataFrame): each row is a polygon representing a chunk\n",
    "    \"\"\"\n",
    "    # check\n",
    "    if not hasattr(da.data, \"chunks\"):\n",
    "        raise ValueError(\"Input array is not chunked (not a Dask array).\")\n",
    "    if 'x' not in da.dims or 'y' not in da.dims:\n",
    "        raise ValueError(\"DataArray must have 'x' and 'y' dimensions.\")\n",
    "\n",
    "    # chunk borders\n",
    "    x_dim = da.dims.index('x')\n",
    "    y_dim = da.dims.index('y')\n",
    "\n",
    "    x_chunks = da.data.chunks[x_dim]\n",
    "    y_chunks = da.data.chunks[y_dim]\n",
    "    x_edges = np.cumsum([0] + list(x_chunks))\n",
    "    y_edges = np.cumsum([0] + list(y_chunks))\n",
    "\n",
    "    x_vals = da['x'].values\n",
    "    y_vals = da['y'].values\n",
    "    y_descending = y_vals[0] > y_vals[-1]\n",
    "\n",
    "    polygons = []\n",
    "    for i in range(len(y_edges)-1):\n",
    "        for j in range(len(x_edges)-1):\n",
    "            xmin = x_vals[x_edges[j]]\n",
    "            xmax = x_vals[x_edges[j+1]-1]\n",
    "            if y_descending:\n",
    "                ymax = y_vals[y_edges[i]]\n",
    "                ymin = y_vals[y_edges[i+1]-1]\n",
    "            else:\n",
    "                ymin = y_vals[y_edges[i]]\n",
    "                ymax = y_vals[y_edges[i+1]-1]\n",
    "            polygons.append(box(xmin, ymin, xmax, ymax))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame({'variable': var_name or getattr(da, 'name', 'unnamed'),\n",
    "                            'geometry': polygons},\n",
    "                           crs=getattr(da.rio, 'crs', None))\n",
    "    return gdf\n",
    "\n",
    "cloud_gdf = get_chunk_polygons(data['cloud'], var_name=\"cloud\")\n",
    "cloud_gdf.to_file(\"data/cloud_chunks.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4daae1d2-ee97-45a9-ad4b-b5906667bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####### PERFORMANCE without chunks\n",
    "    # Loading collection (NZ26 tile, cloud + scl, without calculations, without chunks, 2024, 720 assets)  - 24m, 25s (1460s)\n",
    "    # Loading collection (HP40 tile, cloud + scl, without calculations, without chunks, 2024, 333 assets) - 10m, 56s\n",
    "\n",
    "    ####### PERFORMANCE WITH CHUNKS(without counting records per pixel per year)\n",
    "    # chunks={'time': 20, 'x': 300, 'y': 300}\n",
    "    # Tile NZ06 - load, mask, calculate, export (cloud + scl, 2024, 290 assets) ~233s\n",
    "    # Tile NZ04 - load, mask, calculate, export (cloud + scl, 2024, 146 assets) ~144s\n",
    "    # Tile NZ26 - load, mask, calculate, export (cloud + scl, 2024, 720 assets) 433s\n",
    "    # Tile NZ46 - load, mask, calculate, export (cloud + scl, 2024, 584 assets) 286s\n",
    "    # Tile NZ44 - load, mask, calculate, export (cloud + scl, 2024, 292 assets) 165s\n",
    "    # Tile NZ24 - load, mask, calculate, export (cloud + scl, 2024, 292 assets) 167s\n",
    "    ###### Other years\n",
    "    # Tile NZ26 (2023) - load, mask, calculate, export (cloud+scl, 713 assets) - 358s\n",
    "    # Tile NZ26 (2022) - load, mask, calculate, export (cloud+scl, 713 assets) ~40s\n",
    "    # Tile NZ26 (2021) - load, mask, calculate, export (cloud+scl, 721 assets) ~411s   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
