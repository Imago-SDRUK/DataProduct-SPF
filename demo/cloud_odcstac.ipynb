{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c7066e-336b-489a-bd7c-55b43b9c4680",
   "metadata": {},
   "source": [
    "This Notebook is the raw code to get the cloud probability product without much description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d8367-076f-46ed-9f69-6a0d0ce32cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install odc-stac\n",
    "# pip install pystac-client\n",
    "# pip install dask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pystac_client import Client\n",
    "import geopandas as gpd\n",
    "import rioxarray as xrx\n",
    "from rioxarray import merge\n",
    "\n",
    "import odc.stac\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "tiles_path=\"data/uk_20km_grid.gpkg\"\n",
    "tiles_of_interest=[\"NZ26\",\"NZ24\",\"NZ04\",\"NZ06\",\"NZ44\",\"NZ46\"]\n",
    "#tiles_of_interest=[\"NZ26\"] # NOTE: replace with the tile name if you want calcs for just a single tile (for example, \"NZ26\")\n",
    "collection=\"sentinel-2-c1-l2a\"\n",
    "year=2020\n",
    "time_of_interest=f'{year}-01-01/{year}-12-31'\n",
    "bands_of_interest=[\"cloud\", \"scl\"]\n",
    "api_url=\"https://earth-search.aws.element84.com/v1\"\n",
    "res=20\n",
    "target_crs=27700\n",
    "target_dtype=\"float32\" #NOTE:we save to GeoTIFF, so dtype must be the same across bands (can't specify integer for the band with pixel count)\n",
    "\n",
    "tiles = gpd.read_file(tiles_path).to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccb848-d081-45b7-83fb-fe36297487b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kernel_limit():\n",
    "    \"\"\"This checks the local kernel limitations on memory.\n",
    "    Prints `-1` if inherited from OS and no restrictions.\"\"\"\n",
    "    import resource\n",
    "\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    print(f\"Address space (virtual memory) soft/hard: {soft}/{hard}\")\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_DATA)\n",
    "    print(f\"Data segment size soft/hard: {soft}/{hard}\")\n",
    "\n",
    "check_kernel_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_search(items,index=0):\n",
    "    \"\"\"Print inspect/debug info about the one of the assets (usually the first one).\n",
    "    Parameters:\n",
    "    items (pystac.ItemCollection): STAC collection\n",
    "    index (int=0): random tile number to inspect (Default: 0)\n",
    "    \"\"\"\n",
    "    item = items[index]\n",
    "    try:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Inspecting the asset #{index}\")\n",
    "        print(f\"DATETIME is {item.datetime}\")\n",
    "        print(f\"GEOMETRY is {item.geometry}\")\n",
    "        print(f\"PROPERTIES are:\\n{item.properties}\")\n",
    "        print(f\"CRS: {item.properties.get('proj:code') or item.properties.get('proj:epsg')}\")\n",
    "        '''metadata=odc.stac.extract_collection_metadata(item, cfg=None, md_plugin=None)\n",
    "        print(f\"STAC metadata:\\n{metadata}\")'''\n",
    "        print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Error checking item[{index}]: {e}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_with_scl(data):\n",
    "    \"\"\"\n",
    "    Replace cloud values based on SCL mask.\n",
    "    Rules:\n",
    "    - if SCL == 0 → cloud = -1\n",
    "    - if SCL != 0 → keep cloud as is\n",
    "    - assign -1 as the new no-data value for the cloud band\n",
    "    - ensure no NaNs remain (fill them with -1)\n",
    "    \n",
    "    Parameters:\n",
    "    data (xarray.Dataset): original dataset with `cloud` and `scl` variables\n",
    "    Returns:\n",
    "    cloud (xarray.DataArray): output array with masked `cloud` band\n",
    "    \"\"\"\n",
    "    print((\"-\" * 40 + \"\\n\") * 2, end=\"\")\n",
    "    print(f\"Starting masking...\")\n",
    "    cloud = data[\"cloud\"].astype(\"float32\").copy()\n",
    "    scl = data[\"scl\"].astype(\"int16\")\n",
    "    \n",
    "    print(f\"Cloud dimensions: {cloud.dims}\")\n",
    "    print(f\"SCL dimensions: {scl.dims}\")\n",
    "    \n",
    "    # apply rules\n",
    "    cloud_masked = cloud.where(scl != 0, -1)\n",
    "\n",
    "    # replace any remaining NaNs with -1\n",
    "    cloud_masked = cloud_masked.fillna(-1)\n",
    "    # assign -1 as the nodata value for output\n",
    "    cloud_masked.attrs[\"nodata\"] = -1\n",
    "\n",
    "    print(\"Type:\", type(cloud_masked))\n",
    "    print(\"Name:\", cloud_masked.name)\n",
    "    print(\"Dimensions:\", cloud_masked.dims)\n",
    "    print(\"Coordinates:\", list(cloud_masked.coords))\n",
    "    print(\"-1 is set as the no-data value (no NaNs remain).\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return cloud_masked\n",
    "\n",
    "    \"\"\"\n",
    "    #NOTE: DEBUG for checking the cloud masked \n",
    "    #NOTE: heavy calculation as it opens the whole numpy array (computes)\n",
    "    num_nodata = cloud_masked.isnull().sum().compute().item()\n",
    "    print(f\"Number of no-data (NaN) values: {num_nodata}\")\n",
    "    \n",
    "    # select the first time slice if 'time' is one of the dimensions\n",
    "    if \"time\" in cloud_masked.dims:\n",
    "        first_scene = cloud_masked.isel(time=0)\n",
    "    else:\n",
    "        first_scene = cloud_masked\n",
    "\n",
    "    # ensure CRS and spatial transform are defined\n",
    "    first_scene = first_scene.rio.write_crs(data[\"cloud\"].rio.crs, inplace=False)\n",
    "    # export to GeoTIFF\n",
    "    output_path = \"cloud_masked_first_scene.tif\"\n",
    "    first_scene.rio.to_raster(output_path)\n",
    "    print(f\"Exported first scene to {output_path}\")\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # DEPRECATED\n",
    "    # define bad pixels\n",
    "    nodatas = bitmask\n",
    "    all_bad_pixels = nodatas(dim=\"time\")\n",
    "\n",
    "    # Expand dimensions to match data shape\n",
    "    all_bad_expanded = all_bad_pixels.broadcast_like(nodatas)\n",
    "\n",
    "    # For these pixels, we’ll override and mark them as good\n",
    "    effective_bad_mask = nodatas.where(~all_bad_expanded, other=False)\n",
    "\n",
    "    # Apply the mask: keep data where bad == False\n",
    "    masked = data.where(~effective_bad_mask)\n",
    "    return masked\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1776bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_geotif(dataset,bands_of_interest:list=None, out_path:str=None):\n",
    "    \"\"\"The function saves the dataset/array with multiple variables to geotiff\"\"\"\n",
    "    # Taken from: https://discourse.pangeo.io/t/comparing-odc-stac-load-and-stackstac-for-raster-composite-workflow/4097\n",
    "\n",
    "    if isinstance(dataset, xr.DataArray):\n",
    "        print(f\"Dataset/array: Array\")\n",
    "        image = dataset.squeeze('year').rio.write_crs(target_crs)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset/array: Dataset\")\n",
    "        image = (\n",
    "            dataset[bands_of_interest]\n",
    "            .to_array(dim=\"band\")\n",
    "            .squeeze('year')\n",
    "            .transpose('band', 'y', 'x')\n",
    "            .rio.write_crs(f\"epsg:{target_crs}\")\n",
    "            .astype(target_dtype)\n",
    "        )\n",
    "        \n",
    "    # DEBUG\n",
    "    \"\"\"\n",
    "    # force computation to see all chunks\n",
    "    image.compute()\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"X extent:\", image['x'].min().item(), image['x'].max().item())\n",
    "    print(\"Y extent:\", image['y'].min().item(), image['y'].max().item())\"\"\"\n",
    "\n",
    "    image_out=image.rio.to_raster(\n",
    "        out_path,\n",
    "        driver=\"COG\",\n",
    "        compress=\"LZW\",\n",
    "        dtype=target_dtype\n",
    "    )\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    return image_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901e871-c8ee-402a-a970-f86b6295b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tile(tiles_of_interest, bbox_of_interest):\n",
    "    \"\"\"\n",
    "    Load STAC items for a single tile\n",
    "    Parameters:\n",
    "    tiles_of_interest (str): name of the tile.\n",
    "    bbox_of_interest (list or tuple): bounding box [minx, miny, maxx, maxy] for the tile\n",
    "    Returns:\n",
    "    xr.Dataset (loaded dataset for the tile)\n",
    "    \"\"\"\n",
    "    print(\"-\" * 40)\n",
    "    print(f'Process for tile {tiles_of_interest} started.', flush=True)\n",
    "    print(f\"Bbox of interest is: {bbox_of_interest}\")\n",
    "\n",
    "    catalog = Client.open(\n",
    "        api_url\n",
    "    )\n",
    "    search = catalog.search(\n",
    "        collections=collection,\n",
    "        bbox=bbox_of_interest,\n",
    "        datetime=time_of_interest\n",
    "    )\n",
    "    items = search.item_collection()\n",
    "    print(f\"Number of items: {len(items)}\")\n",
    "    print(f\"Type of items: {type(items)}\")\n",
    "    inspect_search(items, index=0)\n",
    "\n",
    "    data = odc.stac.stac_load(\n",
    "        items,\n",
    "        bands=bands_of_interest,\n",
    "        bbox=bbox_of_interest,\n",
    "        resolution=res,\n",
    "        crs=target_crs,\n",
    "        chunks={'time': 20, 'x': 300, 'y': 300}\n",
    "    ) \n",
    "    #NOTE: parameters to align the data borders: `like`, `anchor` or `geobox`\n",
    "    # NOTE: odc.stac.stac_load is not covered by the documentation yet \n",
    "    # (only the old version - odc.stac.load in 0.39.0)\n",
    "    # TO check the actual documentation: `help(odc.stac.stac_load)`\n",
    "\n",
    "    print(\"Dataset has been loaded\")\n",
    "    print(data)\n",
    "    print(data.dims)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c661c-54cd-481e-ad18-ce61bf1feaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: version with cloud_prob, masked by scl\n",
    "# NOTE: do not skip `.where(cloud-masked!=-1)` because otherwise it just won't consider pixels covered by satellite, but with a -1 cloud probability\n",
    "# NOTE: we use now `-1` instead of 0 because that's our new no data value\n",
    "def process_cloud (cloud_masked):\n",
    "    \"\"\"This function calculates the median cloud probability, counts valid pixels from the xarray dataset, and then export it to GeoTIFFs.\n",
    "    Parameters:\n",
    "    dataset (xarray.Dataset): dataset with `cloud` and `scl` variables\n",
    "    out_path (str): path to the output GeoTIFFs.\n",
    "    Returns:\n",
    "    cloud_out (xarray.Dataset): output xarray dataset with calculated median probability and valid pixels\n",
    "    \"\"\"\n",
    "    cloud_out = xr.Dataset({\n",
    "        \"cloud_prob\": (\n",
    "            cloud_masked\n",
    "            .where(cloud_masked != -1)\n",
    "            .groupby(\"time.year\")\n",
    "            .mean(dim=\"time\", skipna=True)\n",
    "            .astype(\"float32\")\n",
    "        ),\n",
    "        \"valid_count\": (\n",
    "            cloud_masked\n",
    "            .where(cloud_masked != -1)\n",
    "            .groupby(\"time.year\")\n",
    "            .count(dim=\"time\")\n",
    "            .astype(\"int16\")\n",
    "            # NOTE: `.count` in xarray automatically calculates without NODATA values\n",
    "        )\n",
    "    })\n",
    "\n",
    "    print(cloud_out)\n",
    "    print(cloud_out.dims)\n",
    "    print(cloud_out.rio.crs)\n",
    "    print(cloud_out.rio.bounds()) # NOTE: bounds are extended\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return cloud_out\n",
    "    # NOTE: occasional issue met loading data from AWS (TODO - to document once faced again)\n",
    "    # Aborting load due to failure while reading: https://e84-earth-search-sentinel-data.s3.us-west-2.amazonaws.com/sentinel-2-c1-l2a/30/U/XG/2024/7/S2A_T30UXG_20240707T112127_L2A/SCL.tif:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa1bee-6aec-47e7-900b-b593c8dc8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_s2_scenes(data, items, output_dir=\"data/test\", band=\"cloud\"):\n",
    "    \"\"\"\n",
    "    Export Sentinel-2 scenes from an xarray Dataset or DataArray to individual GeoTIFFs.\n",
    "    Useful for visual checks within a short timeframe, eg January images.\n",
    "    Filenames are based on the STAC 's2:tile_id' property.\"\"\"\n",
    "    \n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # extract tile IDs from STAC items\n",
    "    tile_ids = [item.properties.get(\"s2:tile_id\", f\"scene_{i}\") for i, item in enumerate(items)]\n",
    "    # attach as the coordinate\n",
    "    if \"tile_id\" not in data.coords:\n",
    "        data = data.assign_coords(tile_id=(\"time\", tile_ids))\n",
    "    # wrap in a dataset if it's dataarray\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        print(\"Input is a DataArray — converting to Dataset for export.\")\n",
    "        data = data.to_dataset(name=data.name or band)\n",
    "    # if band exists\n",
    "    if band not in data.data_vars:\n",
    "        raise ValueError(f\"Band '{band}' not found in dataset. Available bands: {list(data.data_vars)}\")\n",
    "        \n",
    "    # loop over scenes\n",
    "    for i, tile_id in enumerate(data.tile_id.values):\n",
    "        print(f\"Processing scene {i+1}/{len(data.time)} → {tile_id}\")\n",
    "\n",
    "        # select one scene and load into memory\n",
    "        scene = data.isel(time=i).compute()\n",
    "        out_path = os.path.join(output_dir, f\"{band}_{tile_id}.tif\")\n",
    "        # to check if crs is written\n",
    "        scene_band = scene[band]\n",
    "        scene_band = scene_band.rio.write_crs(scene_band.rio.crs or data[band].rio.crs, inplace=False)\n",
    "        \n",
    "        scene_band.rio.to_raster(out_path)\n",
    "        print(f\"Exported: {out_path}\")\n",
    "\n",
    "    print(f\"\\n All {len(data.time)} scenes exported to '{output_dir}'.\")\n",
    "\n",
    "# USAGE (to export unmasked scenes)\n",
    "# export_s2_scenes(data, items, output_dir=\"data/test/unmasked\", band=\"cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN PROCESS\n",
    "# The block below utilises functions either for a single tile, or for a tile series:\n",
    "# 1. load tile (with inspection)\n",
    "# 2. mask cloud with scl\n",
    "# (optional) export masked and unmasked scenes to debug (for short datetime)\n",
    "# 3. calculate cloud probability and valid count\n",
    "# (optional) mosaic cloud probability, if multiple tiles\n",
    "# 4. export to geotiff\n",
    "\n",
    "if tiles_of_interest and len(tiles_of_interest) == 1: #Single tile\n",
    "    print(f\"Calculating cloud probability for a tile of interest {tiles_of_interest}...\")\n",
    "    tile_name = tiles_of_interest[0]\n",
    "\n",
    "    selected_tile = tiles[tiles[\"tile_name\"] == tile_name]\n",
    "    bbox_of_interest = selected_tile.total_bounds.tolist()\n",
    "    \n",
    "    data=load_tile(tile_name, bbox_of_interest)\n",
    "\n",
    "    #DEBUG: check each scene (for test dataset) - unmasked\n",
    "    #export_s2_scenes(data, items, output_dir=\"data/test/unmasked\", band=\"cloud\")\n",
    "\n",
    "    try:\n",
    "        cloud_masked = mask_with_scl(data)\n",
    "        print(f\"Cloud masked with SCL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to mask cloud with SCL band\")\n",
    "\n",
    "    #DEBUG: check each scene (for test dataset) - masked\n",
    "    #export_s2_scenes(cloud_masked, items, output_dir=\"data/test/masked\", band=\"cloud\")\n",
    "\n",
    "    cloud_out=process_cloud(cloud_masked)\n",
    "\n",
    "    out_path = f'data/{tile_name}_{year}.tif'\n",
    "    out_bands=['cloud_prob','valid_count']\n",
    "    try:\n",
    "        to_geotif(cloud_out,out_bands, out_path=out_path)\n",
    "        print(f\"Output GeoTIFF saved to {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save GeoTIFF: {e}\")\n",
    "\n",
    "else: #Tile series\n",
    "    print(f\"Calculating cloud probability for a tile series {tiles_of_interest}...\")\n",
    "    processed_tiles = []\n",
    "     \n",
    "    for tile_name in tiles_of_interest:\n",
    "        selected_tile = tiles[tiles[\"tile_name\"] == tile_name]\n",
    "        bbox_of_interest = selected_tile.total_bounds.tolist()\n",
    "\n",
    "        data=load_tile(tile_name, bbox_of_interest)\n",
    "\n",
    "        cloud_masked = mask_with_scl(data)\n",
    "\n",
    "        cloud_out=process_cloud(cloud_masked)\n",
    "        print(f\"CLOUD OUT dimensions: {cloud_out.dims}\")\n",
    "        cloud_out.attrs[\"tile_name\"] = tile_name # NOTE: write the tilename to the dataset attributes\n",
    "        \n",
    "        processed_tiles.append(cloud_out) #NOTE: should be fine as it's a lazy operation\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        \"\"\"\n",
    "        out_path = f'data/{tile_name}_2024.tif'\n",
    "        out_bands=['cloud_prob','valid_count']\n",
    "        \n",
    "        try:\n",
    "            to_geotif(cloud_out,out_bands, out_path=out_path)\n",
    "            print(f\"Output GeoTIFF saved to {out_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save GeoTIFF: {e}\")\n",
    "        \"\"\"\n",
    "    # mosaic files together\n",
    "    \n",
    "    first_tile = processed_tiles[0].attrs.get('tile_name')\n",
    "    mosaic = processed_tiles[0]\n",
    "    print(f\"Starting mosaic with the first tile {first_tile}\")\n",
    "    for tile in processed_tiles[1:]:\n",
    "        dataset_name=tile.attrs.get('tile_name')\n",
    "        print(f\"Combining tiled dataset {dataset_name} into mosaic for the base...\")\n",
    "        mosaic = tile.combine_first(mosaic) # NOTE: this will combine the next tiles with the first one\n",
    "    # NOTE: Dask will raise a warning for yearly mosaic:\n",
    "    # /opt/conda/lib/python3.12/site-packages/dask/array/core.py:4996: PerformanceWarning: Increasing number of chunks by factor of 16\n",
    "    # result = blockwise(\n",
    "    out_path=f'data/mosaic_{year}.tif'\n",
    "    out_bands=['cloud_prob','valid_count']\n",
    "    print(\"Combining finished.\")\n",
    "    # NOTE: xr.merge (processed_tiles, compat=\"override\") didn't work - it's exporting the GeoTIFF with the merged extent, but overriding all pixels of other datasets (tiles) with NaN\n",
    "          \n",
    "    try:\n",
    "        to_geotif(mosaic,out_bands, out_path=out_path)\n",
    "        print(f\"Output GeoTIFF saved to {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save GeoTIFF: {e}\")\n",
    "    # NOTE: mainly time spent on GeoTIFF exporting, especially mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED: DEBUG check of the output dataset (unique values, 0 values, no data values) - this is all very heavy for Dask computations\n",
    "\"\"\"cloud_prob_2d = cloud_prob.squeeze('year')  # shape: (y, x)\n",
    "unique_values = np.unique(cloud_prob_2d.values)\n",
    "print(f\"Number of unique values: {len(unique_values)}\")\n",
    "print(\"First 20 unique values:\", unique_values[:20])\n",
    "num_zeros = np.sum(cloud_prob_2d.values == 0)\n",
    "total_pixels = cloud_prob_2d.size\n",
    "percent_zeros = num_zeros / total_pixels * 100\n",
    "print(f\"Number of pixels with value 0: {num_zeros}\")\n",
    "print(f\"Percent of zeros: {percent_zeros:.2f}%\")\n",
    "print(cloud_prob.shape)\"\"\"\n",
    "\n",
    "'''\n",
    "cloud_prob_2d = cloud_prob.squeeze('year')  # shape: (y, x)\n",
    "num_zeros = np.sum(cloud_prob_2d.values == 0)\n",
    "total_pixels = cloud_prob_2d.size\n",
    "percent_zeros = num_zeros / total_pixels * 100\n",
    "print(f\"Number of pixels with value 0: {num_zeros}\")\n",
    "print(f\"Percent of zeros: {percent_zeros:.2f}%\")\n",
    "\n",
    "# squeeze year dimension if single year\n",
    "cloud_prob = cloud_prob.squeeze('year')\n",
    "\n",
    "# convert variables to float32\n",
    "cloud_prob = cloud_prob.astype('float32')\n",
    "\n",
    "# set spatial dims for rioxarray\n",
    "cloud_prob = cloud_prob.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
    "# set CRS\n",
    "cloud_dataset = cloud_prob.rio.write_crs(27700)\n",
    "\n",
    "out_path = \"data/cloud_median_count_2024.tif\"\n",
    "cloud_dataset.rio.to_raster(out_path)\n",
    "print(f\"Saved multi-band GeoTIFF to {out_path}\")'''\n",
    "\n",
    "'''\n",
    "tif = xrx.open_rasterio(\"data/NZ26_2024.tif\", masked=True)  # masked=True treats nodata as NaN\n",
    "num_nodata = int(tif.isnull().sum())\n",
    "total_pixels = tif.size\n",
    "percent_nodata = num_nodata / total_pixels * 100\n",
    "\n",
    "print(f\"GeoTIFF shape: {tif.shape}\")\n",
    "print(f\"NoData pixels (NaN): {num_nodata}\")\n",
    "print(f\"Percent NoData: {percent_nodata:.2f}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f22029-0bb5-478a-adaf-a4d7ff76a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - decide how to handle cropping tiles in mosaic (as odc stac returns arrays with extended borders)\n",
    "\"\"\"\n",
    "def crop_array(array, bbox):\n",
    "\"Crops the output array by the initial bounding box.\n",
    "Parameters:\n",
    "xarray.DataArray: output array\n",
    "bbox: initial bounding box\n",
    "Returns:\n",
    "xarray.DataArray: cropped array\n",
    "    .rio.clip_box() # to crop by the initial bounding box\n",
    "    fa\n",
    "crop_array(cloud_prob,bbox)    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac46cf3-afa5-4677-9ba4-1ce281a044c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset(array):\n",
    "    \"\"\"General info about the output array\"\"\"\n",
    "    print(type(array))\n",
    "    print(array.shape)\n",
    "    print(array.coords)\n",
    "    print(array.attrs)\n",
    "    print(array.dims)\n",
    "    num_nodata = array.isnull().sum()\n",
    "    print(f\"Total nodata values: {num_nodata}\")\n",
    "\n",
    "check_dataset(cloud_out)\n",
    "# TODO - to add logic for checking Dataset, in addition to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105c24b-1da0-45d5-b5d4-130b9723cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_chunks(obj):\n",
    "    \"\"\"\n",
    "    Inspects Dask chunking for an xarray Dataset or DataArray.\n",
    "    Prints total number of chunks, average size, and alignment info.\n",
    "    \"\"\"\n",
    "    # Handle Dataset (multiple variables)\n",
    "    if isinstance(obj, xr.Dataset):\n",
    "        print(f\"Dataset with {len(obj.data_vars)} variables:\")\n",
    "        print(\"=\" * 60)\n",
    "        for var_name, da in obj.data_vars.items():\n",
    "            print(f\"\\nVariable: {var_name}\")\n",
    "            inspect_chunks(da)\n",
    "        return\n",
    "\n",
    "    da = obj #handle dataarray (single variable)\n",
    "\n",
    "    if not hasattr(da.data, \"chunks\"):\n",
    "        print(\"Array not chunked (not a Dask array).\")\n",
    "        return\n",
    "\n",
    "    chunks = da.data.chunks\n",
    "    dtype_size = da.dtype.itemsize\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    total_chunks = 1\n",
    "    uneven = False\n",
    "\n",
    "    for dim, sizes in zip(da.dims, chunks):\n",
    "        total_chunks *= len(sizes)\n",
    "        equal = len(set(sizes)) == 1\n",
    "        if not equal:\n",
    "            uneven = True\n",
    "        print(f\"{dim:>6}: {len(sizes)} chunks | sizes = {sizes[:5]}{'...' if len(sizes) > 5 else ''}\")\n",
    "\n",
    "    avg_chunk_elems = np.prod([np.mean(s) for s in chunks])\n",
    "    avg_chunk_bytes = avg_chunk_elems * dtype_size\n",
    "    avg_chunk_mb = avg_chunk_bytes / 1e6\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    print(f\"Average chunk size: {avg_chunk_mb:.2f} MB ({da.dtype})\")\n",
    "    print(f\"Chunks evenly sized? {'Yes' if not uneven else 'No, uneven chunks'}\")\n",
    "\n",
    "inspect_chunks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2cad1-97b7-4c7a-ab2f-d2ad3b613de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "def get_chunk_polygons(da, var_name=None):\n",
    "    \"\"\"\n",
    "    Generate a geodataframe with polygons for each Dask chunk of a dataarray.\n",
    "    Parameters:\n",
    "    da (xr.DataArray): dask-backed DataArray with 'x' and 'y' coordinates\n",
    "    var_name (str, optional): variable name for labeling in the geodataframe\n",
    "    Returns:\n",
    "    gdf (geopandas.GeoDataFrame): each row is a polygon representing a chunk\n",
    "    \"\"\"\n",
    "    # check\n",
    "    if not hasattr(da.data, \"chunks\"):\n",
    "        raise ValueError(\"Input array is not chunked (not a Dask array).\")\n",
    "    if 'x' not in da.dims or 'y' not in da.dims:\n",
    "        raise ValueError(\"DataArray must have 'x' and 'y' dimensions.\")\n",
    "\n",
    "    # chunk borders\n",
    "    x_dim = da.dims.index('x')\n",
    "    y_dim = da.dims.index('y')\n",
    "\n",
    "    x_chunks = da.data.chunks[x_dim]\n",
    "    y_chunks = da.data.chunks[y_dim]\n",
    "    x_edges = np.cumsum([0] + list(x_chunks))\n",
    "    y_edges = np.cumsum([0] + list(y_chunks))\n",
    "\n",
    "    x_vals = da['x'].values\n",
    "    y_vals = da['y'].values\n",
    "    y_descending = y_vals[0] > y_vals[-1]\n",
    "\n",
    "    polygons = []\n",
    "    for i in range(len(y_edges)-1):\n",
    "        for j in range(len(x_edges)-1):\n",
    "            xmin = x_vals[x_edges[j]]\n",
    "            xmax = x_vals[x_edges[j+1]-1]\n",
    "            if y_descending:\n",
    "                ymax = y_vals[y_edges[i]]\n",
    "                ymin = y_vals[y_edges[i+1]-1]\n",
    "            else:\n",
    "                ymin = y_vals[y_edges[i]]\n",
    "                ymax = y_vals[y_edges[i+1]-1]\n",
    "            polygons.append(box(xmin, ymin, xmax, ymax))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame({'variable': var_name or getattr(da, 'name', 'unnamed'),\n",
    "                            'geometry': polygons},\n",
    "                           crs=getattr(da.rio, 'crs', None))\n",
    "    return gdf\n",
    "\n",
    "cloud_gdf = get_chunk_polygons(data['cloud'], var_name=\"cloud\")\n",
    "cloud_gdf.to_file(\"data/cloud_chunks.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d77066-06e4-492a-b782-3ab04cc5e29b",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "#### Without chunks\n",
    "| Tile | Year | Assets | Operation                                | Duration         |\n",
    "| ---- | ---- | ------ | ---------------------------------------- | ---------------- |\n",
    "| NZ26 | 2024 | 720    | Load only (cloud + SCL, no calculations) | 24m 25s (1460s)  |\n",
    "| HP40 | 2024 | 333    | Load only (cloud + SCL, no calculations) | 10m 56s          |\n",
    "\n",
    "#### With chunks (without counting records per pixel per year)\n",
    "`chunks={'time': 20, 'x': 300, 'y': 300}`\n",
    "| Tile | Year | Assets | Operation                        | Duration |\n",
    "| ---- | ---- | ------ | -------------------------------- | -------- |\n",
    "| NZ06 | 2024 | 290    | Load + Mask + Calculate + Export | 233s     |\n",
    "| NZ04 | 2024 | 146    | Load + Mask + Calculate + Export | 144s     |\n",
    "| NZ26 | 2024 | 720    | Load + Mask + Calculate + Export | 433s     |\n",
    "| NZ46 | 2024 | 584    | Load + Mask + Calculate + Export | 286s     |\n",
    "| NZ44 | 2024 | 292    | Load + Mask + Calculate + Export | 165s     |\n",
    "| NZ24 | 2024 | 292    | Load + Mask + Calculate + Export | 167s     |\n",
    "\n",
    "#### With chunks + pixel counts (the same specifications)\n",
    "Same specs, but possibly cached queries as the time might be random\n",
    "| Tile | Year | Assets | Operation                        | Duration |\n",
    "| ---- | ---- | ------ | -------------------------------- | -------- |\n",
    "| NZ06 | 2024 | 290    | Load + Mask + Calculate + Export | 32s      |\n",
    "| NZ04 | 2024 | 146    | Load + Mask + Calculate + Export | 164s     |\n",
    "| NZ26 | 2024 | 720    | Load + Mask + Calculate + Export | 400s     |\n",
    "| NZ46 | 2024 | 584    | Load + Mask + Calculate + Export | 172s     |\n",
    "| NZ44 | 2024 | 292    | Load + Mask + Calculate + Export | 218s     |\n",
    "| NZ24 | 2024 | 292    | Load + Mask + Calculate + Export | 185s     |\n",
    "\n",
    "#### Other years\n",
    "| Tile | Year | Assets | Operation                        | Duration |\n",
    "| ---- | ---- | ------ | -------------------------------- | -------- |\n",
    "| NZ26 | 2023 | 713    | Load + Mask + Calculate + Export | 358s     |\n",
    "| NZ26 | 2022 | 713    | Load + Mask + Calculate + Export | 40s      |\n",
    "| NZ26 | 2021 | 721    | Load + Mask + Calculate + Export | 411s     |\n",
    "\n",
    "However, processing duration will be completely different if output datasets are not exported to separate GeoTiffs, but instead combined into one mosaic:\n",
    "| Tile          | Year | Assets, sum of tiles| Operation                        | Duration |\n",
    "| ------------- | ---- | ------------------- | -------------------------------- | -------- |\n",
    "| Newcastle (6) | 2024 | 2324                | Load + Mask + Calculate + Mosaic | 1049s    |\n",
    "| Newcastle (6) | 2023 | 2312                | Load + Mask + Calculate + Mosaic | 1590s    |\n",
    "| Newcastle (6) | 2022 | 187                 | Load + Mask + Calculate + Mosaic | 29s      |\n",
    "| Newcastle (6) | 2021 | 2324                | Load + Mask + Calculate + Mosaic | 898s     |\n",
    "| Newcastle (6) | 2020 | 2296                | Load + Mask + Calculate + Mosaic | ....s    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785f3b6-9701-4f8e-97fd-42283b8a77e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
