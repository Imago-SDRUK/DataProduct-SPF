{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOUD PROBABILITY PRODUCT\n",
    "\n",
    "In this notebook we explore how to create an annual cloud probability product using Sentinel-2 satellite imagery data. We describe how to use various parameters and configurations to obtain raw band data and product.\n",
    "\n",
    "To calculate the sun exposure/sunniness/daylight (?) index, we would have to use many spatio-temporal variables, including the area cloudiness, or probability of retrieveing clouds in the particular point in space and time.\n",
    "\n",
    "Py-STAC solution is based on the [carpentries guide](https://carpentries-incubator.github.io/geospatial-python/05-access-data.html).\n",
    "This Notebook has been partially based on the [Sentinel Hub API example](https://sentinelhub-py.readthedocs.io/en/latest/examples/process_request.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "We use the base [Docker image](https://hub.docker.com/r/behzad89/geo-miniconda3) to run the Notebook and [helper functions](raster_utils.py) and also install [`pystac-client`](https://pystac-client.readthedocs.io/en/stable/tutorials/authentication.html) to access data.\n",
    "\n",
    "### Auxiliary data\n",
    "To assist in the computation, we'll use two small input datasets, [located here](data/):\n",
    "- boundary of the Tyne and Wear area (Newcastle agglomeration) from [here](link?)\n",
    "- British National Grid, which covers the whole UK from [here](https://github.com/OrdnanceSurvey/OS-British-National-Grids?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~The access token is obtained through Copernicus Data Ecosystem: https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html#:~:text=Client%20in%20your-,account%20settings,-.%20This%20is%20so~~\n",
    "\n",
    "~~Here is the instruction to register a token: https://documentation.dataspace.copernicus.eu/APIs/SentinelHub/Overview/Authentication.html~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Py-stac solution\n",
    "\n",
    "Let's configure our pystac client first, find out all available collections and stick to one of them. It has been found out that cloud probabilities are described by the Sentinel-2 Collection 1 L2A product.\n",
    "\n",
    "Usually, the cloud probability products are available through Sentinel Hub API which is subject to significant access restrictions. Although many hubs provide free access to Sentinel-2 products, they do not include additional bands, such as a cloud probability (Copernicus Data Space Ecosystem, Microsoft Planetary). However, Element84 (Earth Search) provides access to cloud probability products to kept on AWS storage. For details, see the registry key ([here](https://registry.opendata.aws/sentinel-2-l2a-cogs/)) and STAC catalogue of the relevant Sentinel-2 collection([here](https://radiantearth.github.io/stac-browser/#/external/earth-search.aws.element84.com/v1/collections/sentinel-2-c1-l2a)).\n",
    "\n",
    "**TODO:** back-up plan if Element84 terminate access. Any limit rate for earth84?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pystac-client if not installed\n",
    "from pystac_client import Client\n",
    "\n",
    "api_url = \"https://earth-search.aws.element84.com/v1\"\n",
    "# OTHER ENDPOINTS TO GET DATA\n",
    "#api_url = \"https://stac.dataspace.copernicus.eu/v1/\" # NOTE: this doesn't contain cloud bands in the assets\n",
    "# https://hub.openeo.org/ (not production ready)\n",
    "client = Client.open(api_url)\n",
    "\n",
    "collections = client.get_collections()\n",
    "for collection in collections:\n",
    "    print(collection)\n",
    "collection = \"sentinel-2-c1-l2a\" # NOTE: do not use \"sentinel-2-l2a\" as it doesn't contain cloud probabilities\n",
    "\n",
    "datetime = '2023-01-01/2023-12-31'\n",
    "\n",
    "# NOTE: Connecting to client might freeze sometimes, kernel restart would help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CLOUD PROBABILITY ACCESS\n",
    "\n",
    "\n",
    "We will download Sentinel-2 imagery of Tyne and Wear Area. Let's try with a just one 20-km tile of the area of interest.\n",
    "These tiles have already been prepared.\n",
    "\n",
    "The bounding box of this tile in `WGS84` coordinate system is `[54.933089, -1.689407, 55.114004, -1.374500]` (longitude and latitude coordinates of lower left and upper right corners).\n",
    "\n",
    "![area_of_interest_tile](illustrations/area_of_interest_tile.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNG visualisation - another way\n",
    "\"\"\"\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(\n",
    "    Image(filename=\"illustrations/area_of_interest_tile.png, width=400),\n",
    "    Image(filename=\"illustrations/nodata_issue_snow_legend.png\", width=250)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "# define polygon vertices as (lat, lon) tuples - correct for Shapely\n",
    "tile = Polygon([\n",
    "    (-1.689407, 54.933089),  # lat, lon swapped\n",
    "    (-1.374500, 54.933089),\n",
    "    (-1.374500, 55.114004),\n",
    "    (-1.689407, 55.114004),\n",
    "    (-1.689407, 54.933089)\n",
    "])\n",
    "\n",
    "search = client.search(\n",
    "    collections=[collection],\n",
    "    intersects=tile,\n",
    "    datetime=datetime\n",
    ")\n",
    "\n",
    "print(f\"Number of matched scenes: {search.matched()}\")\n",
    "\n",
    "# TODO - to write a function to transform the tile extent into Shapely polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a tile 20x20 km in the UK we will usually have hundreds of acquisitions per year. For example, for the sample tile we got 713 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = search.item_collection()\n",
    "print(f\"Number of items: {len(items)}\")\n",
    "for item in items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inspection, let's check out one of the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 250\n",
    "item = items[index]\n",
    "try:\n",
    "    print(item.datetime)\n",
    "    print(item.geometry)\n",
    "    print(item.properties)\n",
    "    print(item.properties.get('proj:code') or item.properties.get('proj:epsg'))\n",
    "except Exception as e:\n",
    "    print(f\"Error checking item[{index}]: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download publicly available bands in the Sentinel collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = item.assets\n",
    "print(assets.keys())\n",
    "print(assets[\"thumbnail\"].href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = assets[\"cloud\"]\n",
    "if cloud is None:\n",
    "    raise KeyError(\"The asset 'cloud' not found\")\n",
    "print(type(cloud))\n",
    "\n",
    "print(cloud.href)   # URL to the asset\n",
    "print(cloud.media_type)\n",
    "print(cloud.roles) # roles might be non-intuitive\n",
    "print(cloud.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORT\n",
    "First, we would like to export the full image locally to check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "cloud_href = assets[\"cloud\"].href\n",
    "cloud = rioxarray.open_rasterio(cloud_href)\n",
    "print(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a note that some scenes might have a small number of valid pixels (see **'STATISTICS_VALID_PERCENT'**). That doesn't mean these pixels are not suitable for follow-up analysis as this attribute describes the entire whole scene area whereas the actual area of interest might have larger share of valid pixels.\n",
    "\n",
    "Moreover, no data values in this product usually mean that these pixels just belong to other non-cloudy categories (eg, vegetation, water, or snow).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole image to disk\n",
    "cloud.rio.to_raster(f\"data/cloud_{index}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also download another band for a visual comparison with cloud probability - SCL, which divide scene by rough 'land-cover' categories, including clouds. As you can see, no data value (0) in the cloud probability product are usually observed in non-cloudy SCL categores, such as vegetation or water.\n",
    "\n",
    "Therefore, no data value in a pixel doesn't mean we can't say for sure if it's a cloud or not - it usually means that it's not a cloud. In other words, `STATISTICS_VALID_PERCENT` is not a quality metric of cloud probability product and doesn't describe accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_href = assets[\"scl\"].href\n",
    "scl = rioxarray.open_rasterio(scl_href)\n",
    "print(scl)\n",
    "\n",
    "# save whole image to disk\n",
    "scl.rio.to_raster(f\"data/scl_{index}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to work with particular tiles. Let's call a separate function which will clip the scene by the extent of the tile of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib # to reload external changes\n",
    "import raster_utils\n",
    "\n",
    "importlib.reload(raster_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the National Grid tiles Tyne and Wear area intersects. For that purpose, we are going to use [20x20km grid](https://github.com/OrdnanceSurvey/OS-British-National-Grids?tab=readme-ov-file). Let's call the external function to find out the intersected tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_path = \"data/NewcastleUponTyne.gpkg\"\n",
    "tile_path = \"data/uk_20km_grid.gpkg\"\n",
    "\n",
    "touched, aoi_crs, tile_crs = raster_utils.touched_tiles(aoi_path, tile_path)\n",
    "print(touched, aoi_crs, tile_crs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will limit calculations with one tile, mentioned above. Let's call the external function to clip the scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cloud=raster_utils.clip_scene_by_one_tile(cloud, touched, index=3)\n",
    "clipped_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you will see the **'STATISTICS_VALID_PERCENT'**, but this attribute inherited value from the initial image and is not correct anymore. If you wish, you can recalculate stats and find out how many non-cloudy pixels you have in your tile of interest now. It should be different from what you found out earlier in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodata_val=clipped_cloud.rio.nodata\n",
    "print(f\"Nodata value is {nodata_val}\")\n",
    "total_pixels=clipped_cloud.size\n",
    "print(f\"Total number of pixels is {total_pixels}\")\n",
    "nodata_pixels=((clipped_cloud == nodata_val).sum().item())\n",
    "print(f\"Number of pixels=0 is {nodata_pixels}\")\n",
    "\n",
    "nodata_share=(nodata_pixels/total_pixels)*100 if total_pixels >0 else 0\n",
    "print(f\"Share of pixels=0 is {nodata_share} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKING NO DATA VALUES\n",
    "The problem is that there are two types of pixels with cloud probability encoded as 0:\n",
    "1. within area covered by satellite, where there are no clouds. We want these pixels!\n",
    "2. outside area covered by satellite, but included in the product image. We don't want these pixels!\n",
    "\n",
    "For the valid calculations, we have to eliminate pixels of type 2.\n",
    "\n",
    "<img src=\"illustrations/nodata_issue.png\" alt=\"nodata_issue\" style=\"width:50%;\">\n",
    "<img src=\"illustrations/nodata_issue_legend.png\" alt=\"nodata_issue_legend\" style=\"width:10%;\">\n",
    "\n",
    "# TODO - to write a function on separating true no data values and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we have the same problem with the similar product - snow probability. Since snow is not particularly common in the Tyne and Wear area, we’ll access a single scene where snow was known to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=\"S2B_T30UWF_20230116T111315_L2A\" # another ID - S2A_T30UWG_20230117T113415_L2A\n",
    "\n",
    "items_by_id = {it.id: it for it in items}\n",
    "item = items_by_id.get(target_id)\n",
    "if item is None:\n",
    "    raise ValueError(f\"Item with id {target_id} not found\")\n",
    "\n",
    "print(item.id)\n",
    "print(item.properties)\n",
    "\n",
    "assets = item.assets\n",
    "print(assets.keys())\n",
    "print(assets[\"thumbnail\"].href)\n",
    "\n",
    "snow = assets[\"snow\"]\n",
    "if snow is None:\n",
    "    raise KeyError(\"The asset 'snow' not found\")\n",
    "print(type(snow))\n",
    "\n",
    "print(snow.href)   # URL to the asset\n",
    "print(snow.media_type)\n",
    "print(snow.roles) # roles might be non-intuitive\n",
    "print(snow.title)\n",
    "\n",
    "import rioxarray\n",
    "snow_href = assets[\"snow\"].href\n",
    "snow = rioxarray.open_rasterio(snow_href)\n",
    "print(snow)\n",
    "\n",
    "# save whole image to disk\n",
    "snow.rio.to_raster(f\"data/snow_{target_id}.tif\")\n",
    "\n",
    "# Number of snowy tile - S2A_T30UWG_20230117T113415_L2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that snow probability product has the same issue. On the sample image you can see two types of pixels=0:\n",
    "1. Pixels not covered by snow (these pixels are useful for further analysis)\n",
    "2. Pixels outside of the satellite coverage, included into the grid, but divided from the snow pixels with a sharp straight line. These pixels are not useful for further analysis!\n",
    "\n",
    "<img src=\"illustrations/nodata_issue_snow.png\" alt=\"nodata_snow_issue\" style=\"width:50%;\">\n",
    "<img src=\"illustrations/nodata_issue_snow_legend.png\" alt=\"nodata_snow_issue_legend\" style=\"width:30%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's export the output locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cloud.rio.to_raster(f\"data/clipped_cloud_{index}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CLOUD PROBABILITY CALCULATION **(ONE tile, ONE year)**\n",
    "\n",
    "So far, everything was relatively simple, wasn't it?\n",
    "\n",
    "Now, to understand how frequent you can enjoy sun (or suffer) in a particular pixel, we can try to extract the average value of cloud probability over a year. \n",
    "\n",
    "First, we could try it for only one tile of interest, for one year.\n",
    "So, we will use all scenes (or timestamps, or items) available as we sure so far it won't take too long to calculate average values across <1000 of images.\n",
    "\n",
    "**NOTE**: some images do not cover tiles entirely as satellites provide images in so-called swaths (MGRS grids), so swaths may slice the area of interest, partly leaving it without values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. First, let's try just loop over all items. \n",
    "We could loop over all STAC items, open them, create one enormously giant array and calculate cloud probability, but that's definitely not the best day to do it.\n",
    "\n",
    "So, we will test another way:\n",
    "* loop over scenes (items)\n",
    "* open first scene and extract value in each pixel\n",
    "* store values in each pixel\n",
    "* open next scene and extract value in each pixel\n",
    "* add value in each pixel to previous value\n",
    "* once all values across all scenes are cumulated, divide the cumulated value by the number of scenes\n",
    "\n",
    "Just be aware before running cells, it may take quite a while.\n",
    "Non-cached calculation for 2023 year (713 scenes) took 29 minutes (all tiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: average snow probability across all items\n",
    "arrays = []\n",
    "print (\"Looping over all scenes (items):\")\n",
    "\n",
    "for item in items:   # your filtered STAC items\n",
    "    cloud_asset = item.assets.get(\"cloud\")\n",
    "    if cloud_asset:\n",
    "        da = rioxarray.open_rasterio(cloud_asset.href, masked=True)\n",
    "        arrays.append(da)\n",
    "\n",
    "# stack them along a new \"time\" dimension\n",
    "stack = xr.concat(arrays, dim=\"time\")\n",
    "print(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the mean average for each pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_raster = stack.mean(dim=\"time\", skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export and visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_raster.rio.to_raster(\"data/snow_mean.tif\")\n",
    "mean_raster.plot(figsize=(6,6), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean_accum = None\n",
    "count = 0\n",
    "print (f\"Looping over all scenes ({len(items)} items):\")\n",
    "\n",
    "for i,item in enumerate(items):\n",
    "    \n",
    "    cloud_asset = item.assets.get(\"cloud\")\n",
    "    if cloud_asset:\n",
    "        da = rioxarray.open_rasterio(cloud_asset.href)\n",
    "        data = da.squeeze().values.astype(\"float32\")\n",
    "\n",
    "        if mean_accum is None:\n",
    "            mean_accum = np.zeros_like(data)\n",
    "            ref_da = da  # keep reference for spatial metadata\n",
    "\n",
    "        mean_accum += data\n",
    "        # print(mean_accum)\n",
    "        count += 1\n",
    "        print(f\"{i} - ID {item.id}\")\n",
    "\n",
    "mean_accum /= count # calculate average # it's numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export and visualise the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = ref_da.rio.transform()\n",
    "crs = ref_da.rio.crs\n",
    "height, width = ref_da.shape[-2:]\n",
    "\n",
    "profile = {\n",
    "    \"driver\": \"GTiff\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"count\": 1,\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"crs\": crs,\n",
    "    \"transform\": transform,\n",
    "    \"compress\": \"lzw\"\n",
    "}\n",
    "\n",
    "with rasterio.open(\"data/cloud_mean.tif\", \"w\", **profile) as dst:\n",
    "    dst.write(mean_accum, 1)\n",
    "\n",
    "print(\"✅ Exported mean raster to data/cloud_mean.tif\")\n",
    "\n",
    "# to drop the 'band' dimension (not needed, we have only one possible band)\n",
    "# wrap numpy array with spatial metadata from referenece\n",
    "mean_da = ref_da.squeeze().copy(data=mean_accum)\n",
    "\n",
    "mean_da.plot(\n",
    "    figsize=(6, 6),\n",
    "    cmap=\"Blues\",\n",
    "    cbar_kwargs={\"label\": \"Mean cloud probability (%)\"}\n",
    ")\n",
    "plt.title(\"Mean yearly cloud probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes an enormous amount of time, isn't it? And that's all just for one year.\n",
    "\n",
    "### TILE OF INTEREST\n",
    "We can test the performance for our tile of interest. Let's define parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# parameters\n",
    "aoi_path = \"data/NewcastleUponTyne.gpkg\"\n",
    "tile_path = \"data/uk_20km_grid.gpkg\"\n",
    "\n",
    "collection=\"sentinel-2-c1-l2a\"\n",
    "datetime='2023-01-01/2023-12-31'\n",
    "asset=\"cloud\"\n",
    "\n",
    "touched, aoi_crs, tile_crs = raster_utils.touched_tiles(aoi_path, tile_path)\n",
    "print(touched)\n",
    "spatial_extent= touched.iloc[[3]].copy()  # double brackets to keep as DataFrame\n",
    "print(spatial_extent) # THAT'S OUR TILE\n",
    "print(type(spatial_extent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate cloud probability, masking the scenes with the tile of interest. We avoid loading the full raster into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
    "import geopandas as gpd\n",
    "\n",
    "mean_accum=None\n",
    "count = 0\n",
    "ref_profile=None\n",
    "print(f\"Looping over all scenes ({len(items)} items):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "spatial_extent.to_file(\"data/spatial_extent.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "for i,item in enumerate(items):\n",
    "    cloud_asset = item.assets.get(\"cloud\")\n",
    "    if cloud_asset:\n",
    "        # da = da.rio.clip(spatial_extent.geometry, spatial_extent.crs, drop=False)  # NOTE: another masking option (still loads everything to memory)\n",
    "        with rasterio.open(cloud_asset.href) as src:            \n",
    "            window = from_bounds(*spatial_extent.total_bounds, transform=src.transform)\n",
    "\n",
    "            data=src.read(1,window=window).astype(\"float32\")\n",
    "\n",
    "            \"\"\"# AOI bounds in raster CRS\n",
    "            minx, miny, maxx, maxy = spatial_extent.total_bounds\n",
    "            print(minx,miny,maxx,maxy)\n",
    "            print(spatial_extent.crs)\n",
    "            print(src.crs)\"\"\"\n",
    "\n",
    "            if spatial_extent.crs != src.crs:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src.crs, spatial_extent.crs, src.width, src.height, *src.bounds\n",
    "                )\n",
    "                data_reproj = np.empty((height, width), dtype=np.float32)\n",
    "\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=data_reproj,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=spatial_extent.crs,\n",
    "                    resampling=Resampling.nearest #NOTE:nearest or bilinear\n",
    "                )\n",
    "\n",
    "                data = data_reproj\n",
    "            else:\n",
    "                data = src.read(1).astype(\"float32\")\n",
    "                #NOTE: crucial because we need cartesian CRS (recorded in tile, not in Sentinel scenes)\n",
    "                # TODO - to reproject rasterio object to spatial extent crs \n",
    "\n",
    "            # NOTE: DEBUG\n",
    "            # AOI bounds in raster CRS\n",
    "            minx, miny, maxx, maxy = spatial_extent.total_bounds\n",
    "            '''print(minx,miny,maxx,maxy)'''\n",
    "            # Raster resolution (pixel size)\n",
    "            res_x, res_y = src.res\n",
    "            width = int((maxx - minx) / res_x)\n",
    "            height = int((maxy - miny) / res_y)\n",
    "            print(f\"AOI width: {width}, height: {height}, resolution: {res_x}\")\n",
    "\n",
    "            print(f\"{i} - ID {item.id}, shape: {data.shape}, bounds: {spatial_extent.total_bounds}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "            # keep reference profile for export\n",
    "            if ref_profile is None:\n",
    "                ref_profile = src.profile.copy()\n",
    "                ref_profile.update({\n",
    "                    \"height\": data.shape[0],\n",
    "                    \"width\": data.shape[1],\n",
    "                    \"transform\": src.window_transform(window),\n",
    "                    \"dtype\": \"float32\",\n",
    "                    \"count\": 1,\n",
    "                    \"compress\": \"lzw\"\n",
    "                })\n",
    "                mean_accum = np.zeros_like(data)\n",
    "                'valid_count = np.zeros_like(data) # TODO - to consider later when no data values and true ZEROs are separated'\n",
    "\n",
    "            mean_accum += data #TODO - shape of arrays is different because they can cover the AOI only partially. Resample to the area of tile?\n",
    "            count += 1\n",
    "        \n",
    "        if mean_accum is None:\n",
    "            mean_accum = np.zeros_like(data)\n",
    "\n",
    "    \n",
    "mean_accum /= count # calculate average # it's numpy ndarray\n",
    "print(f\"Processed {len(items)} scenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the time required is comparable to previous calculation because computational performance probably depends mostly on the loading scenes, not on calculating the cloud probability. Still, we have to download the full scenes, and calculating the cloud probability in the smaller area doesn't help much to reduce computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ref_da.rio.transform()\n",
    "crs = ref_da.rio.crs\n",
    "height, width = ref_da.shape[-2:]\n",
    "\n",
    "profile = {\n",
    "    \"driver\": \"GTiff\",\n",
    "    \"dtype\": \"float32\",\n",
    "    \"count\": 1,\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"crs\": crs,\n",
    "    \"transform\": transform,\n",
    "    \"compress\": \"lzw\"\n",
    "}\n",
    "\n",
    "with rasterio.open(\"data/cloud_mean_bbox.tif\", \"w\", **profile) as dst:\n",
    "    dst.write(mean_accum, 1)\n",
    "\n",
    "print(\"Exported mean raster to data/cloud_bbox.tif\")\n",
    "\n",
    "# to drop the 'band' dimension (not needed, we have only one possible band)\n",
    "# wrap numpy array with spatial metadata from referenece\n",
    "\n",
    "# Clip or resize mean_accum to ref_da shape\n",
    "mean_accum = mean_accum.reshape(ref_da.shape)\n",
    "\n",
    "mean_accum_cropped = mean_accum[:ref_da.shape[0], :ref_da.shape[1]]\n",
    "\n",
    "mean_da = ref_da.copy(data=mean_accum_cropped)\n",
    "\n",
    "\n",
    "mean_da.plot(\n",
    "    figsize=(6, 6),\n",
    "    cmap=\"Blues\",\n",
    "    cbar_kwargs={\"label\": \"Mean cloud probability (%), clipped by a single tile\"}\n",
    ")\n",
    "plt.title(\"Mean yearly cloud probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking the results, we can spot some issues in data product with buildings roofs - some warehouses and industrial scenes with white roofs tend to have the higher cloud probability.\n",
    "\n",
    "<img src=\"illustrations/issue_roofs.png\" alt=\"roofs_issue\" style=\"width:80%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO - RASTER STATS\n",
    "\n",
    "It would be nice to provide a quick statistical analysis of cloud probability distribution. For example, range of values is not large because clouds persistently cover all of the pixels - there are no regions where clouds are very rare (at first glance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Parallelised calculation\n",
    "\n",
    "# TODO - Test Dask locally for CPU parallelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPRECATED - Sentinel Hub API (Process API)\n",
    "\n",
    "~~Process API requires Sentinel Hub account. Please check [configuration instructions](https://sentinelhub-py.readthedocs.io/en/latest/configure.html) about how to set up your Sentinel Hub credentials.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif not config.sh_client_id or not config.sh_client_secret:\\n    print(\"Warning! To use Process API, please provide the credentials (OAuth client ID and client secret).\")\\n\\nif config.sh_client_id and config.sh_client_secret:\\n    print(\"config found\")\\n\\nprint(config)\\n\\n# to check where the configuration is \\nSHConfig.get_config_location()'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentinelhub import SHConfig\n",
    "\"\"\"\n",
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "# it's recommended to install sentinelhub[AWS]: pip install sentinelhub[AWS]\n",
    "from requests_oauthlib import OAuth2Session\n",
    "config = SHConfig()\n",
    "\n",
    "\n",
    "CLIENT_ID = \"...\"\n",
    "CLIENT_SECRET = \"...\"\n",
    "TOKEN_URL = \"https://services.sentinel-hub.com/auth/realms/main/protocol/openid-connect/token\"\n",
    "\n",
    "# set up credentials\n",
    "client = BackendApplicationClient(client_id=CLIENT_ID)\n",
    "oauth = OAuth2Session(client=client)\n",
    "\n",
    "# get an authentication token\n",
    "token = oauth.fetch_token(\n",
    "    token_url=TOKEN_URL,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "if not config.sh_client_id or not config.sh_client_secret:\n",
    "    print(\"Warning! To use Process API, please provide the credentials (OAuth client ID and client secret).\")\n",
    "\n",
    "if config.sh_client_id and config.sh_client_secret:\n",
    "    print(\"config found\")\n",
    "\n",
    "print(config)\n",
    "\n",
    "# to check where the configuration is \n",
    "SHConfig.get_config_location()\"\"\"\n",
    "\n",
    "# TODO - to find out how adjust config.toml file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEPRECATED - EARTH DAILY:\n",
    "\n",
    "~~Let's also try Earth Daily STAC:\n",
    "ACCOUNT TO BE CREATED YET (https://console.earthdaily.com/platform/signin)~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_new_token(session):\\n    \\'\\'\\'Obtain a new authentication token using client credentials.\\'\\'\\'\\n    token_req_payload = {\"grant_type\": \"client_credentials\"}\\n    try:\\n        token_response = session.post(EDS_AUTH_URL, data=token_req_payload)\\n        token_response.raise_for_status()\\n        tokens = token_response.json()\\n        return tokens[\"access_token\"]\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Failed to obtain token: {e}\")\\n\\ntoken = get_new_token(session)\\n\\nclient = Client.open(STAC_API_URL, headers={\"Authorization\": f\"bearer {token}\"}) \\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pystac.client import Client\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "CLIENT_ID = os.getenv(\"EDS_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"EDS_SECRET\")\n",
    "EDS_AUTH_URL = os.getenv(\"EDS_AUTH_URL\")\n",
    "API_URL = os.getenv(\"EDS_API_URL\")\n",
    "STAC_API_URL = f\"{API_URL}/platform/v1/stac\"\n",
    "\n",
    "# Setup requests session\n",
    "session = requests.Session()\n",
    "session.auth = (CLIENT_ID, CLIENT_SECRET)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def get_new_token(session):\n",
    "    '''Obtain a new authentication token using client credentials.'''\n",
    "    token_req_payload = {\"grant_type\": \"client_credentials\"}\n",
    "    try:\n",
    "        token_response = session.post(EDS_AUTH_URL, data=token_req_payload)\n",
    "        token_response.raise_for_status()\n",
    "        tokens = token_response.json()\n",
    "        return tokens[\"access_token\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to obtain token: {e}\")\n",
    "\n",
    "token = get_new_token(session)\n",
    "\n",
    "client = Client.open(STAC_API_URL, headers={\"Authorization\": f\"bearer {token}\"}) \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
